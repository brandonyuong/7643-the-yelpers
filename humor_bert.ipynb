{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "hundred-referral",
   "metadata": {},
   "source": [
    "# Classifying Humorous Text from Yelp Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "recognized-chrome",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "extreme-mason",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify device:\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increased-piece",
   "metadata": {},
   "source": [
    "## Load & Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stunning-wrong",
   "metadata": {},
   "source": [
    "##### Download dataset from https://www.kaggle.com/yelp-dataset/yelp-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "altered-trace",
   "metadata": {},
   "source": [
    "## Split into Training, Validation, and Test Sets\n",
    "\n",
    "#####  Run etl_1 and etl_2 before starting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "surface-carrier",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 9)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.read_csv(\"./data/yelp_reduced.csv\")\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "confident-earth",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(result.text, \n",
    "                                                    result.humor, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=8, \n",
    "                                                    stratify=result.humor)\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, \n",
    "                                                random_state=8, \n",
    "                                                test_size=0.5, \n",
    "                                                stratify=y_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "injured-price",
   "metadata": {},
   "source": [
    "## Import Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "refined-temperature",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import BERT-base pretrained model\n",
    "bert = AutoModel.from_pretrained('bert-base-cased')\n",
    "\n",
    "MODEL_TYPE = 'bert-base-cased'\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seasonal-mountain",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "meaning-bearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rest of the notebook is adapted from \n",
    "# https://github.com/prateekjoshi565/Fine-Tuning-BERT/blob/master/Fine_Tuning_BERT_for_Spam_Classification.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "victorian-heater",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get length of all the messages in the train set\n",
    "seq_len = [len(i.split()) for i in X_train]\n",
    "\n",
    "pd.Series(seq_len).hist(bins = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "peaceful-legislation",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "worthy-dispute",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and encode sequences in the training set\n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    X_train.tolist(),\n",
    "    max_length = max_seq_len,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the validation set\n",
    "tokens_val = tokenizer.batch_encode_plus(\n",
    "    X_val.tolist(),\n",
    "    max_length = max_seq_len,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the test set\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    X_test.tolist(),\n",
    "    max_length = max_seq_len,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-novelty",
   "metadata": {},
   "source": [
    "## Convert to Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "digital-dominant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for train set\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(y_train.tolist())\n",
    "\n",
    "# for validation set\n",
    "val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "val_y = torch.tensor(y_val.tolist())\n",
    "\n",
    "# for test set\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "test_y = torch.tensor(y_test.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "talented-stewart",
   "metadata": {},
   "source": [
    "## Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "curious-crowd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#define a batch size\n",
    "batch_size = 32\n",
    "\n",
    "# wrap tensors\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "# sampler for sampling the data during training\n",
    "train_sampler = RandomSampler(train_data)\n",
    "# dataLoader for train set\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# wrap tensors\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "\n",
    "# dataLoader for validation set\n",
    "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fiscal-hebrew",
   "metadata": {},
   "source": [
    "## Freeze BERT Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "medical-gnome",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all the parameters\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immune-missouri",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "alien-dynamics",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_Arch(nn.Module):\n",
    "\n",
    "    def __init__(self, bert):\n",
    "      \n",
    "      super(BERT_Arch, self).__init__()\n",
    "\n",
    "      self.bert = bert \n",
    "      self.dropout = nn.Dropout(0.1)\n",
    "      self.relu =  nn.ReLU()\n",
    "      self.fc1 = nn.Linear(768,512)\n",
    "      self.fc2 = nn.Linear(512,2)\n",
    "      self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, sent_id, mask):\n",
    "\n",
    "      #pass the inputs to the model  \n",
    "      _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n",
    "      x = self.fc1(cls_hs)\n",
    "      x = self.relu(x)\n",
    "      x = self.dropout(x)\n",
    "      # output layer\n",
    "      x = self.fc2(x)\n",
    "      # apply softmax activation\n",
    "      x = self.softmax(x)\n",
    "\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "confidential-lecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the pre-trained BERT to our define architecture\n",
    "model = BERT_Arch(bert)\n",
    "\n",
    "# push the model to GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "desperate-compact",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer from hugging face transformers\n",
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uniform-procedure",
   "metadata": {},
   "source": [
    "## Get Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abandoned-remains",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "#compute the class weights\n",
    "class_wts = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "\n",
    "print(class_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "electoral-jason",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class weights to tensor\n",
    "weights= torch.tensor(class_wts,dtype=torch.float)\n",
    "weights = weights.to(device)\n",
    "\n",
    "# loss function\n",
    "cross_entropy  = nn.CrossEntropyLoss(weight=weights) \n",
    "\n",
    "# number of training epochs\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nutritional-bermuda",
   "metadata": {},
   "source": [
    "## Fine-tune BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "rough-berry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train the model\n",
    "def train():\n",
    "  \n",
    "  model.train()\n",
    "\n",
    "  total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "  # empty list to save model predictions\n",
    "  total_preds=[]\n",
    "  \n",
    "  # iterate over batches\n",
    "  for step,batch in enumerate(train_dataloader):\n",
    "    \n",
    "    # progress update after every 50 batches.\n",
    "    if step % 50 == 0 and not step == 0:\n",
    "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "\n",
    "    # push the batch to gpu\n",
    "    batch = [r.to(device) for r in batch]\n",
    " \n",
    "    sent_id, mask, labels = batch\n",
    "\n",
    "    # clear previously calculated gradients \n",
    "    model.zero_grad()        \n",
    "\n",
    "    # get model predictions for the current batch\n",
    "    preds = model(sent_id, mask)\n",
    "\n",
    "    # compute the loss between actual and predicted values\n",
    "    loss = cross_entropy(preds, labels)\n",
    "\n",
    "    # add on to the total loss\n",
    "    total_loss = total_loss + loss.item()\n",
    "\n",
    "    # backward pass to calculate the gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    # model predictions are stored on GPU. So, push it to CPU\n",
    "    preds=preds.detach().cpu().numpy()\n",
    "\n",
    "    # append the model predictions\n",
    "    total_preds.append(preds)\n",
    "\n",
    "  # compute the training loss of the epoch\n",
    "  avg_loss = total_loss / len(train_dataloader)\n",
    "  \n",
    "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "  # reshape the predictions in form of (number of samples, no. of classes)\n",
    "  total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "  #returns the loss and predictions\n",
    "  return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "irish-blanket",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for evaluating the model\n",
    "def evaluate():\n",
    "  \n",
    "  print(\"\\nEvaluating...\")\n",
    "  \n",
    "  # deactivate dropout layers\n",
    "  model.eval()\n",
    "\n",
    "  total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "  # empty list to save the model predictions\n",
    "  total_preds = []\n",
    "\n",
    "  # iterate over batches\n",
    "  for step,batch in enumerate(val_dataloader):\n",
    "    \n",
    "    # Progress update every 50 batches.\n",
    "    if step % 50 == 0 and not step == 0:\n",
    "      \n",
    "      # Calculate elapsed time in minutes.\n",
    "      elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "      # Report progress.\n",
    "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
    "\n",
    "    # push the batch to gpu\n",
    "    batch = [t.to(device) for t in batch]\n",
    "\n",
    "    sent_id, mask, labels = batch\n",
    "\n",
    "    # deactivate autograd\n",
    "    with torch.no_grad():\n",
    "      \n",
    "      # model predictions\n",
    "      preds = model(sent_id, mask)\n",
    "\n",
    "      # compute the validation loss between actual and predicted values\n",
    "      loss = cross_entropy(preds,labels)\n",
    "\n",
    "      total_loss = total_loss + loss.item()\n",
    "\n",
    "      preds = preds.detach().cpu().numpy()\n",
    "\n",
    "      total_preds.append(preds)\n",
    "\n",
    "  # compute the validation loss of the epoch\n",
    "  avg_loss = total_loss / len(val_dataloader) \n",
    "\n",
    "  # reshape the predictions in form of (number of samples, no. of classes)\n",
    "  total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "  return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ahead-armstrong",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "portuguese-novelty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 10\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.806\n",
      "Validation Loss: 0.711\n",
      "\n",
      " Epoch 2 / 10\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.659\n",
      "Validation Loss: 0.653\n",
      "\n",
      " Epoch 3 / 10\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.628\n",
      "Validation Loss: 0.651\n",
      "\n",
      " Epoch 4 / 10\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.630\n",
      "Validation Loss: 0.631\n",
      "\n",
      " Epoch 5 / 10\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.613\n",
      "Validation Loss: 0.713\n",
      "\n",
      " Epoch 6 / 10\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.715\n",
      "Validation Loss: 0.806\n",
      "\n",
      " Epoch 7 / 10\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.619\n",
      "Validation Loss: 0.695\n",
      "\n",
      " Epoch 8 / 10\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.560\n",
      "Validation Loss: 0.659\n",
      "\n",
      " Epoch 9 / 10\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.634\n",
      "Validation Loss: 0.706\n",
      "\n",
      " Epoch 10 / 10\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.634\n",
      "Validation Loss: 0.895\n"
     ]
    }
   ],
   "source": [
    "# set initial loss to infinite\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "\n",
    "#for each epoch\n",
    "for epoch in range(epochs):\n",
    "     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    \n",
    "    #train model\n",
    "    train_loss, _ = train()\n",
    "    \n",
    "    #evaluate model\n",
    "    valid_loss, _ = evaluate()\n",
    "    \n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "    \n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latin-liberal",
   "metadata": {},
   "source": [
    "## Load Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "prompt-skirt",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load weights of best model\n",
    "path = 'saved_weights.pt'\n",
    "model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "announced-authentication",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "architectural-inspector",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions for test data\n",
    "with torch.no_grad():\n",
    "  preds = model(test_seq.to(device), test_mask.to(device))\n",
    "  preds = preds.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "sweet-update",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.60      0.65        30\n",
      "           1       0.66      0.77      0.71        30\n",
      "\n",
      "    accuracy                           0.68        60\n",
      "   macro avg       0.69      0.68      0.68        60\n",
      "weighted avg       0.69      0.68      0.68        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model's performance\n",
    "preds = np.argmax(preds, axis = 1)\n",
    "print(classification_report(test_y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "normal-rental",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0   0   1\n",
       "row_0        \n",
       "0      18  12\n",
       "1       7  23"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confusion matrix\n",
    "pd.crosstab(test_y, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bright-airline",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "talented-failing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # taking the raw outputs of the stacked encoders of BERT, and attaching their own specific model to it, most commonly \n",
    "# # a linear layer, and then fine-tuning this model on their specific dataset\n",
    "\n",
    "# from transformers import BertModel\n",
    "\n",
    "# class Bert_Model(nn.Module):\n",
    "#    def __init__(self, class):\n",
    "#        super(Bert_Model, self).__init__()\n",
    "#        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "#        self.out = nn.Linear(self.bert.config.hidden_size, classes)\n",
    "#        self.sigmoid = nn.Sigmoid()\n",
    "#    def forward(self, input):\n",
    "#        _, output = self.bert(**input)\n",
    "#        out = self.out(output)\n",
    "#        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otherwise-fever",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
